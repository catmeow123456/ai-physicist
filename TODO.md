## 简介
AI 物理学家由一个总环境和若干个知识模块组成，总环境存储一切变量知识，涉及实验数据的具体计算需要在这个总环境里进行。每个知识模块存储一类相关的，或者与某个实验（或者关键词）挂钩的知识，它负责知识信息的整理、存储、传递。所有的知识模块构成一个巨大的知识库，通过赋予每一条知识权重，使得知识库可以被持续地学习和维护。

## 未来规划

两步走：

**目标 1**. 让程序能在 10 个以上的实验上持续地可靠地运行，知识库不断地迭代更新，
    <!-- 给所有概念添加权重，用机器学习的方法在学习过程中调整权重，进行持续地学习。 -->
在过程中可以生成大量数据（用于机器学习的训练）。

减少人类设计的偏好，
尽量避免路径依赖，保证充分长时间后 AI 一定能发现它应该发现的东西。

**目标 2**. 定义什么是**状态，动作，奖励**。用于机器学习：在状态 s 上，选择执行什么动作 a。因为有了大量数据，就可以用于训练一定复杂的网络。

- 在监督学习中，用 loss function 定义什么是“像”
- 泛化误差上界公式（VC 维理论）

    Low training error + 
    more training data than "degree of freedom" 
    = low test error
    $$
    \rm{Pr}_{S \sim D} \left[\rm{Test}_D(f) - \rm{Train}_S(f) \le 
    \sqrt{\frac{log|F|+log(\frac{1}{\delta})}{|S|}},\quad \forall f \in F\right] > 1 - \delta
    $$
    这里 $F = \{ f : X \to Y\}$ 代表神经网络的各种可能，一般地 $\log|F|$ 正比于网络的独立的参数量。

- **力大砖飞**的原理：
    增大网络自由度 $\log|F|$ 以获得低的训练误差。 
    增大 $|S|$ 来减小泛化误差（避免过拟合）。
https://zhuanlan.zhihu.com/p/565617662


## 数据结构 Theorist 的设计



## 可以在两种情况下测试程序
1.  0 -> 1 : AI 从什么都不知道开始，发现基本概念，发现新知识
2.  n -> n + 1 : 给定一定的知识（给定质量、守恒律），利用给定的知识发现新知识（多弹簧实验中的守恒律）。

## Small TODOs
0.（完成） `aiphysicist.pyi` 存根文件。
1.（完成） 知识库的存储、读取。
2. 内禀概念判重还没做，挑选量程大的内禀概念定义方式。
3. 维护概念历史权重，用 UCB 公式探索性地抽取概念，以获得较大的回报。
4.（完成） reduce conclusions （Grobner 方法）流程优化，将内禀概念的判重也纳入到这个流程中来。
5. 不需要把所有发现的关系都加入 concept（可以只添加那些 reduce conclusions 剩下的那些 conclusion 作为概念）。
6. 新概念如果比旧概念定义更简单，需要进行替换，这个还没做。
7. 发现的守恒量与之前发现的守恒量做 PCA 寻找它们之间的非线性关系（例如 ma - kx = kr + kl）。
8. PCA 找守恒量
